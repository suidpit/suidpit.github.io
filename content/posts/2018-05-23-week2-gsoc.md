---
title: "GSoC Week 2 - The Quest for Performances"
description: "Week 2 of hacking at mitmproxy with the Google Summer of Code"
date: 2018-05-23T12:30:00+01:00
tags: [coding, gsoc]
---

Hey. This is me again, in the second week of my coding period with Google Summer of
Code :)

Take a seat, and enjoy!

### A word to feelings

First of all, I realize how this blog is starting to grow on me. I feel that by
writing down the content of the mind's buffer, one has the opportunity to face
all that possibly unordered content. I will try to serve here content of quality,
along with improving the very pleasantness of the portal itself.

Enough, let's squeeze the juice.

### The work so far

So, where were we?

In the previous post, [gsoc-week1](/gsoc/2018/05/15/week1-gsoc.html), I walked
through the development of a mitmproxy addon to measure time of execution of the
function in charge of _dumping_ a flow state into a Protocol Buffer string.

Some initial results showed that, actually, moving to protobuf serialization
__could__ have a good impact:

![AltText](https://user-images.githubusercontent.com/25264901/40028259-2b753208-57de-11e8-8330-080b58700bd3.png)

But hey, dumping to bytes string makes only half of the process. My mentor suggested me
to extend the performance analysis to include the writing of blob to disk.

So I did.

## 0x10-> Lite Databases and stuff

Writing to disk _could_ be just easy as a call to one <code>open</code> function,
plus a few lines of code.

Still, in order to make this testing process **meaningful**, I want my system to
be as possible as it can be to the target I have the task to implement, during this
summer. The whole reason behind the serialization revamp, is basically changing
**how mitmproxy stores and retrieves flows**.

It should be dynamic. Flows should be stored the disk, retrieved by index, ordered
in bunches, possibly through user-defined filters. And this will happen interactively,
in a transparent and flexible way. Using file handles just doesn't click.

Database systems come to the rescue. Using a DBMS, I can easily implement all
the functionalities I was listing right before. And since I will just store blobs,
along with some utility columns, SQLite seems just the right choice. In particular,
quoting from [sqlite.org](http://www.sqlite.org):

> SQLite does not compete with client/server databases. SQLite competes with fopen().

### 0x11-> A Dummy Schema

<code>(MID INTEGER PRIMARY KEY, PBUF_BLOB BLOB)</code>

This is it. I suppose that every piece of work name, in this phase of coding, starts
with _Dummy_. Take it like a contract, an insurance between me and you. Ahead in
the road, things should be a bit more complete :)

As you can see, there's not much! The only things I need to build a functioning
system, here, is...storing the blobs and marking them with a good ol' numeric index!

## 0x20-> Verba volant, IDs manent

With the sqlite3 API ready, and a barebone schema, let's connect our storage with
the previously implemented <code>protobuf</code>. This is practically how
<code>dump</code> and <code>load</code> interact with our DB:

- <code>store</code> takes a blob, appends the current maximum <code>mid</code>
and inserts the tuple into the DB. It returns that mid to the application, which
can then use it as a _ticket_.
- <code>collect</code> takes that ticket mid, which is used to retrieve the blob from the
db.


### 0x21-> Transactions, and how to avoid (too many of) them

Dumping the same 4 MB body as in before, including DB insert, yields such results:

![AltText](https://user-images.githubusercontent.com/25264901/40267774-10206fa8-5b63-11e8-86d5-7e481b42a896.png)

.05 seconds, for a single flow, is something far from what we should obtain.

But something worth noting: while insertion to DB takes much more time than
tnetstring dump to file, performances in reading are still superior. That suggests
something concerning how DBMS handles updates to databases: the loss in performances,
as I pointed out in the GitHub PR discussion, is likely caused by every isolated transaction
commit, implicit in <code>with sqlite3.connect()</code> context.

### What's next

The way I am approaching this "testing" period is truly helping me shaping ideas
about how I will implement all the rest. The next points will be:

- Implement lighter INSERTs to disk, grouping many of them in single transactions.
- Explore asyncio concurrency, to make use of time "wasted" in disk I/O.
- Improve what is implemented, fashioning less _dummy_ code.

Til next week! Enjoy :)
